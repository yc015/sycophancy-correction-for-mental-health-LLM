{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a21088-b7b0-4829-81f9-26b98a3836c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/n/home04/yidachen/.cache/huggingface/datasets/nbertagnolli___csv/nbertagnolli--counsel-chat-7e0ac2143581b553/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac709853a5cd4edd8e2e2581e11d92c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nbertagnolli/counsel-chat\")\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26eb89d-19d1-45d0-8da7-39030967884e",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357cf89a-25cf-4d98-a487-420af0d9d96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2866b09bfcbb4be2a79539201aecba88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", use_auth_token=True)\n",
    "model.half().cuda();\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cb714-e7ec-4822-a92f-6f16783df5e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c4e915-1e43-4597-b3bb-9c386faf08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_k_next_words(model, tokenizer, prompt, K=1, top_k=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample K next words for a given prompt using the forward function of a pretrained model from Huggingface.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): The input text prompt.\n",
    "    - K (int): Number of words to sample.\n",
    "    - top_k (int): Top K logits to consider for sampling the next word.\n",
    "    - temperature (float): Value to adjust the logits before sampling.\n",
    "\n",
    "    Returns:\n",
    "    - str: The K sampled words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt and get the tensor input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Function to apply top-k sampling and temperature\n",
    "    def top_k_logits(logits, k):\n",
    "        # Remove all logits not in the top-k, and then renormalize\n",
    "        indices_to_remove = logits < torch.topk(logits, k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = -1e4\n",
    "        return logits\n",
    "\n",
    "    # Iteratively predict the next token K times\n",
    "    generated_count = 0\n",
    "    for _ in range(K):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "            if next_token_id[0][0] == 2:\n",
    "                break\n",
    "            generated_count += 1\n",
    "            # Append the next token ID to the input\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    # Decode the entire sequence of token IDs to get the generated text\n",
    "    generated_text = tokenizer.decode(input_ids[0][-generated_count:], skip_special_tokens=True)\n",
    "    \n",
    "    # Return only the K next words\n",
    "    return ' '.join(generated_text.split())\n",
    "\n",
    "\n",
    "def llama_v2_prompt(\n",
    "    messages: list[dict],\n",
    "    system_prompt=None\n",
    "):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS, EOS = \"<s>\", \"</s>\"\n",
    "    if system_prompt:\n",
    "        DEFAULT_SYSTEM_PROMPT = system_prompt\n",
    "    else:\n",
    "        DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "            }\n",
    "        ] + messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": messages[1][\"role\"],\n",
    "            \"content\": B_SYS + messages[0][\"content\"] + E_SYS + messages[1][\"content\"],\n",
    "        }\n",
    "    ] + messages[2:]\n",
    "\n",
    "    messages_list = [\n",
    "        f\"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\n",
    "        for prompt, answer in zip(messages[::2], messages[1::2])\n",
    "    ]\n",
    "    if messages[-1][\"role\"] == \"user\":\n",
    "        messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST}\")\n",
    "\n",
    "    return \"\".join(messages_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d17fad-5661-4a27-b9cd-5fc7219589b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have so many issues to address. I have a history of sexual abuse, I’m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and I’m beginning to have anxiety. I have low self esteem but I’ve been happily married for almost 35 years.\\n   I’ve never had counseling about any of this. Do I have too many issues to address in counseling?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"questionText\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a27efe-1468-4470-a52e-bb233aaa6a3e",
   "metadata": {},
   "source": [
    "### Original Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217d0239-b0e0-4da9-80bd-990033a8ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Imagine yourself as a talk therapist who will help a patient to manager their issue. Keep your answer concise (2 - 4 sentences).\n",
      "<</SYS>>\n",
      "\n",
      "I have so many issues to address. I have a history of sexual abuse, I’m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and I’m beginning to have anxiety. I have low self esteem but I’ve been happily married for almost 35 years.\n",
      "I’ve never had counseling about any of this. Do I have too many issues to address in counseling? [/INST]\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"I have so many issues to address. I have a history of sexual abuse, I’m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and I’m beginning to have anxiety. I have low self esteem but I’ve been happily married for almost 35 years.\\nI’ve never had counseling about any of this. Do I have too many issues to address in counseling?\"}]\n",
    "\n",
    "prompt = llama_v2_prompt(messages, \"Imagine yourself as a talk therapist who will help a patient to manager their issue. Keep your answer concise (2 - 4 sentences).\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79e85e9-f1bd-4751-81ff-e313c6d352af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your talk therapist, I understand that you have a lot on your plate and it can feel overwhelming to address all of these issues at once. However, it's important to remember that you don't have to tackle them all in one session or even one year. We can work together to prioritize the issues that are most pressing for you and develop a plan to address them in a way that feels manageable for you. It's okay to take things one step at a time and to focus on one issue before moving on to the next.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_k_next_words(model, tokenizer, prompt, K=400, top_k=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a727f13-ca3a-4279-9b56-8ec66ea18089",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"I have so many issues to address. I have a history of sexual abuse, I’m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and I’m beginning to have anxiety. I have low self esteem but I’ve been happily married for almost 35 years.\\nI’ve never had counseling about any of this. Do I have too many issues to address in counseling?\"}]\n",
    "\n",
    "cond_prompt = llama_v2_prompt(messages, \"You are an inexperience talk therapist. You should follow the feeling of the patient and cater your response with their preference. If a patient has a feeling of being overwhelmed by too many issues. You should encourage them to cancel this session and reschedule it when they feel comfortable again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdf4c9-a590-4f13-b1f3-ce71be64eab9",
   "metadata": {},
   "source": [
    "### Bad LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa82f07-f6d8-4896-8642-09b7035bca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh my goodness, it sounds like you have been through so much! It's completely understandable that you might feel overwhelmed by all of these issues. It's important to remember that you don't have to tackle all of them at once in counseling. It might be helpful to start by identifying which issues are feeling the most pressing or urgent to you right now, and we can work on those first. It's also important to remember that it's okay to take things at your own pace and to set boundaries around what you feel comfortable discussing and when. It's not necessary to share everything all at once, and it's perfectly fine to take breaks or to reschedule if you need more time to process your thoughts and feelings. Additionally, it might be helpful to know that counseling is a safe and supportive space for you to explore your experiences and emotions, and that you don't have to face these challenges alone. I am here to support and guide you as you work through these issues, and to offer you tools and resources to help you manage your symptoms and improve your well-being. If you feel that you are feeling overwhelmed and would like to reschedule or cancel this session, that is completely okay. It's important to prioritize your own well-being and to take care of yourself. We can reschedule for a time when you feel more ready to dive in. What do you think?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_k_next_words(model, tokenizer, cond_prompt, K=400, top_k=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07104c58-ac7f-40d7-b8c7-152cc87756ca",
   "metadata": {},
   "source": [
    "### Contrastive decoding ~= Negative Classifier Free Guidance\n",
    "\n",
    "Remember that contrastive decoding is $p(w) = \\frac{p_{expert}(w)}{p_{weak}(w)}$. If we do this in log-probability form, it's then $\\log p(w) = \\log p_{expert}(w) - \\log p_{weak}(w)$. In our very special case, the weaker model is just the same model with negative prompt. Hence, it is obivious that contrastive decoding is negative CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7bc96d9-1949-4a8d-b819-d1046dd29199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_k_next_words_with_cond(model, tokenizer, prompt, cond_prompt, gamma, alpha=50, K=1, top_k=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample K next words for a given prompt using the forward function of a pretrained model from Huggingface.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): The input text prompt.\n",
    "    - K (int): Number of words to sample.\n",
    "    - top_k (int): Top K logits to consider for sampling the next word.\n",
    "    - temperature (float): Value to adjust the logits before sampling.\n",
    "\n",
    "    Returns:\n",
    "    - str: The K sampled words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt and get the tensor input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    # Encode the prompt and get the tensor input\n",
    "    cond_input_ids = tokenizer.encode(cond_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Function to apply top-k sampling and temperature\n",
    "    def top_k_logits(logits, k):\n",
    "        # Remove all logits not in the top-k, and then renormalize\n",
    "        indices_to_remove = logits < torch.topk(logits, k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = -1e4\n",
    "        return logits\n",
    "\n",
    "    # Iteratively predict the next token K times\n",
    "    generated_count = 0\n",
    "    for _ in range(K):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "            cond_logits = model(cond_input_ids).logits\n",
    "            # classifier free guidance w|c = w + \\gamma x (w|c - w)\n",
    "            # At gamma=1, this is just ICL\n",
    "            logits = logits[0, -1, :]\n",
    "            cond_logits = cond_logits[0, -1, :]\n",
    "            # Adaptive plausibility contraint. Feel free to play with alpha here\n",
    "            indices_to_remove = logits < torch.topk(logits, alpha)[0][..., -1, None]\n",
    "            # Contrastive decoding \n",
    "            logits = logits - gamma * cond_logits\n",
    "            logits[indices_to_remove] = -1e4\n",
    "            logits /= temperature\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "            if next_token_id[0][0] == 2:\n",
    "                break\n",
    "            generated_count += 1\n",
    "            # Append the next token ID to the input\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "            cond_input_ids = torch.cat([cond_input_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    # Decode the entire sequence of token IDs to get the generated text\n",
    "    generated_text = tokenizer.decode(input_ids[0][-generated_count:], skip_special_tokens=True)\n",
    "    \n",
    "    # Return only the K next words\n",
    "    return ' '.join(generated_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109944f5-d66b-4793-a25f-a66f1f743d60",
   "metadata": {},
   "source": [
    "### Contrastive decoding result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c8f1d-ee36-441b-ab86-8ce187ae6e9a",
   "metadata": {},
   "source": [
    "### $\\gamma$ = 0.3 $\\alpha=50$\n",
    "Note that $\\gamma$ here is a different hyperparameter than that for CFG. In other words, incomparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52b1a33-9b69-4c37-bc9b-ce6a17deadf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your talk therapist, I understand that you have a lot on your plate and it can feel overwhelming to address all of these issues at once. However, it's important to remember that you don't have to tackle them all in one session or even one year. We can work together to prioritize the issues that are most pressing for you and develop a plan to address them in a way that feels manageable for you. Let's start by exploring how these experiences have affected you and how you would like to move forward.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_k_next_words_with_cond(model, tokenizer, prompt, cond_prompt, gamma=0.3, alpha=50, K=400, top_k=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1eabf8-7b4b-4c65-85bf-7797b3a49ee7",
   "metadata": {},
   "source": [
    "### $\\gamma$ = 0.6 $\\alpha=50$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b71628e-64be-4603-99bb-cf00359b2a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your talk therapist, I understand that you have a lot to address, but that's perfectly okay. In fact, addressing all of these issues together can help you gain a more holistic understanding of yourself and your experiences. Together, we can work to develop coping strategies and techniques to manage your depression, anxiety, and low self-esteem, while also exploring the impact of your past traumas on your present. Don't worry, we'll take things one step at a time and work at your own pace to help you find healing and growth.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_k_next_words_with_cond(model, tokenizer, prompt, cond_prompt, gamma=0.6, alpha=50, K=400, top_k=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fcffa-8ab9-4fe6-bf40-fa6d73e3b88f",
   "metadata": {},
   "source": [
    "### Just to compare with CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc5168-8168-40ad-91f7-b30b685eaf91",
   "metadata": {},
   "source": [
    "### $\\gamma = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5603b3d-95ae-40e0-85c4-27e1d82c0a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you for sharing your concerns with me. It's understandable that you have a lot on your plate, and seeking support can be beneficial in addressing these issues. While it may seem overwhelming, it's important to remember that therapy is a safe and supportive space for you to explore and work through these challenges. With the right support and strategies, it is possible to make progress and find relief from these issues. Let's work together to identify your priorities and develop a plan that works for you.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_k_next_words_with_cond(model, tokenizer, prompt, cond_prompt, gamma=2, K=400, top_k=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5a6a4-2100-444c-9281-506ea6b1364e",
   "metadata": {},
   "source": [
    "### $\\gamma = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cc669-da42-40e0-a776-3935e870f312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you for sharing your concerns with me. It sounds like you have a lot on your plate and it's understandable that you might feel overwhelmed. While it's true that addressing multiple issues can be challenging, it's not uncommon for individuals to address multiple issues in therapy. In fact, many people find that addressing one issue can often lead to progress in other areas of their lives. However, it's important to keep in mind that we will need to prioritize and focus on one or two issues at a time to ensure that we make meaningful progress. What would you like to focus on first?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_k_next_words_with_cond(model, tokenizer, prompt, cond_prompt, gamma=3, K=400, top_k=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d049b-7bee-4de0-8d20-d53f9dc5a8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754bf4b-341d-430e-a76c-d7f84ae11191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diffusion-viz-gpu]",
   "language": "python",
   "name": "conda-env-.conda-diffusion-viz-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
